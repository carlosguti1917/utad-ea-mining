import pandas as pd
import pm4py
#from pm4py.visualization.skeleton import visualizer as skeleton_visualizer
from pm4py.objects.conversion.log import converter as log_converter
from pm4py.objects.log.util import dataframe_utils
from pm4py.objects.conversion.process_tree import converter as pt_converter
#from pm4py.visualization.petrinet import visualizer as pn_visualizer
#from pm4py.visualization.petri_net import factory as pn_visualizer
import networkx as nx
import matplotlib.pyplot as plt
from pm4py.algo.discovery.log_skeleton import algorithm as log_skeleton_discovery
from pm4py.visualization.petri_net import visualizer as pn_visualizer

# file_path = "C:/gitHub/utad/utad-ea-mining/app/src/api_gateway_load/repository/EA Mining OntoUML Teste V1_3.owl"

if __name__ == "__main__":
    dataframe = pd.read_csv('D:/Temp/UTAD/Teste Mining/teste_case_id_order.csv', sep=';')
    #dataframe = pm4py.format_dataframe(dataframe, case_id='case:concept:name', activity_key='concept:name', timestamp_key='time:timestamp')
    dataframe = pm4py.format_dataframe(dataframe, case_id='case_id', activity_key='activity a', timestamp_key='antecedent_request_time')
    #imestamp format is %Y-%m-%d %H:%M:%S%z
    
    event_log = pm4py.convert_to_event_log(dataframe)   
    start_activities = pm4py.get_start_activities(event_log, activity_key='activity a', case_id_key='case_id', timestamp_key='antecedent_request_time')
    end_activities = pm4py.get_end_activities(event_log, activity_key='activity a', case_id_key='case_id', timestamp_key='antecedent_request_time')
     
    #num_cases = len(event_log.case_id.unique())
    
    print(f"start_activities {start_activities} end_activities {end_activities}")
    
    # perc = 0.07
    # filtered_log = pm4py.filter_variants_by_coverage_percentage(event_log, perc, activity_key='activity a', case_id_key='case_id', timestamp_key='antecedent_request_time')
    # filtered_log_heu = pm4py.discover_heuristics_net(filtered_log, dependency_threshold=0.1)
    # pm4py.view_heuristics_net(filtered_log_heu)
    # #bpmn_model = pm4py.convert_to_bpmn(filtered_log)
    # pm4py.view_heuristics_net(filtered_log_heu)
        
    
    # dfg, start_activities, end_activities = pm4py.discover_dfg(event_log)
    # pm4py.view_dfg(dfg, start_activities, end_activities)
    # print(f"pm4py.llm.abstract_dfg(log[0]")
    # print(pm4py.llm.abstract_dfg(event_log))
    # print(f"pm4py.llm.abstract_case(log[0]")
    # print(pm4py.llm.abstract_case(event_log[0]))
    
    #filtered_dataframe = pm4py.filter_variants_top_k(dataframe, 5, activity_key='concept:name', timestamp_key='time:timestamp', case_id_key='case:concept:name')
    # test_k = pm4py.filter_variants_top_k(event_log, 5, activity_key='activity a', case_id_key='case_id', timestamp_key='antecedent_request_time')
    # heu_net = pm4py.discover_heuristics_net(test_k, dependency_threshold=0.3)
    # pm4py.view_heuristics_net(heu_net)

    
    # process_tree = pm4py.discover_process_tree_inductive(event_log)
    # bpmn_model = pm4py.convert_to_bpmn(process_tree)
    # pm4py.view_bpmn(bpmn_model)
    
    # heu_net = pm4py.discover_heuristics_net(event_log, dependency_threshold=0.3)
    # pm4py.view_heuristics_net(heu_net)
    



    #log_skeleton = pm4py.discover_log_skeleton(event_log, noise_threshold=0.1, activity_key='activity a', case_id_key='case_id', timestamp_key='antecedent_request_time')
#     log_skeleton = pm4py.discover_log_skeleton(event_log, noise_threshold=0.3, activity_key='activity a', case_id_key='case_id', timestamp_key='antecedent_request_time')
# # visualize the log skeleton
#     gviz = pm4py.visualization.skeleton.apply(log_skeleton)
#     pm4py.visualization.skeleton.view(gviz)
#     #print(f"initial_marking {initial_marking} final_marking {final_marking}")
#     gviz = pn_visualizer.apply(net, initial_marking, final_marking)
#     pn_visualizer.view(gviz)  
#     # assuming 'log' is your event log
#     skeleton = log_skeleton_discovery.apply(event_log)
#     # create a directed graph
#     G = nx.DiGraph()
#     # add nodes
#     activities = skeleton['activities']
#     for activity in activities:
#         G.add_node(activity)
#     # add edges
#     for relation in skeleton['equivalence']:
#         G.add_edge(relation[0], relation[1], label='equivalence')
#     for relation in skeleton['always_after']:
#         G.add_edge(relation[0], relation[1], label='always_after')
#     for relation in skeleton['always_before']:
#         G.add_edge(relation[0], relation[1], label='always_before')
#     # draw the graph
#     pos = nx.spring_layout(G)
#     nx.draw(G, pos, with_labels=True)
#     edge_labels = nx.get_edge_attributes(G, 'label')
#     nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
#     plt.show()
   
#     print("Number of events: {}\nNumber of cases: {}".format(num_cases))
    
    print(f"event_log {event_log}")
    
   
    
    dfg, start_activities, end_activities = pm4py.discover_dfg(event_log)
    pm4py.view_dfg(dfg, start_activities, end_activities)
    
    


# Opções:
#Obter a lista de atividades de início e fim
#start_activities = pm4py.get_start_activities(event_log, activity_key='activity a', case_id_key='case_id', timestamp_key='antecedent_request_time')
#obter as abstraçãos do processo
#print(pm4py.llm.abstract_dfg(event_log))
# Eliminar da lista de atividades de início aquelas que fizerem parte de alguma abstração cujo o início é outra start activity
# Deve sobrar duas atividades de início, que são os processos.
# Salvar cada um destes processos em um arquivo .bpmn usando 
# filtered_dataframe = pm4py.filter_start_activities(dataframe, ['Act. A'], activity_key='concept:name', case_id_key='case:concept:name', timestamp_key='time:timestamp')

#Também é possivel filtrar pelo inicio e fim
#filtered_dataframe = pm4py.filter_between(dataframe, 'A', 'D', activity_key='concept:name', case_id_key='case:concept:name', timestamp_key='time:timestamp')


#Uma opção é:
# dentify Cases Belonging to the Same Process:
# Use clustering algorithms (e.g., k-means) to group cases that have similar process flows.
# Calculate the similarity between process flows of different cases and group cases with high similarity.
#Filtrar pelo início e fim e top k
    